\begin{document}
\sloppy

\title{Systèmes Linéaires et Décompositions Matricielles}
\maketitle

\section{Introduction aux Systèmes Linéaires}

\subsection{Présentation du problème}
On s'intéresse à la résolution de systèmes d'équations linéaires de la forme :
\[ Ax = b \]
où $A \in \mathbb{C}^{N \times N}$ est une matrice carrée de taille $N$, $x \in \mathbb{C}^N$ est le vecteur des inconnues et $b \in \mathbb{C}^N$ est le vecteur second membre.

\subsection{Lien avec la Méthode de Newton}
Dans un contexte plus général, pour résoudre une équation non linéaire $F(x) = 0$ avec $F: \mathbb{R}^N \to \mathbb{R}^N$, on utilise la méthode de Newton. La formule de récurrence est donnée par :
\[ x_{n+1} = x_n - F'(x_n)^{-1} F(x_n) \]
Ici, $F'(x_n)$ représente la matrice jacobienne de $F$ évaluée en $x_n$. À chaque itération, le calcul de $x_{n+1}$ nécessite la résolution d'un système linéaire de la forme $F'(x_n) \delta x = -F(x_n)$.

\section{Propriétés de l'inversibilité}

\subsection{Équivalences pour le cas $A \in \mathbb{C}^{N \times N}$}
Pour une matrice carrée $A$, l'existence et l'unicité de la solution pour tout second membre ($\exists! Ax = b, \forall b$) sont équivalentes aux propriétés suivantes :
\begin{itemize}
    \item $A$ est bijective.
    \item $A$ est injective (par le théorème du rang, cela équivaut à la surjectivité pour une matrice carrée).
    \item $\det A \neq 0$.
    \item $0$ n'est pas une valeur propre (v.p.) de $A$.
\end{itemize}

\subsection{Cas de la matrice non-inversible}
Si $A$ est carrée mais non-inversible, la solution (si elle existe) n'est pas unique.
Une solution existe ($\exists \text{ sol}$) si et seulement si $b$ appartient à l'image de $A$ :
\[ b \in \text{Im } A = \text{Run } A = \text{Vect}(A_1, \dots, A_n) \subseteq \text{Span} \]
où $A_i$ sont les colonnes de la matrice $A$.

\begin{example}
Considérons le système suivant :
\[ 
\begin{cases} 
2x + 4y = \alpha \\
4x + 8y = \beta 
\end{cases} 
\]
L'image de la matrice $A = \begin{pmatrix} 2 & 4 \\ 4 & 8 \end{pmatrix}$ est l'ensemble des vecteurs $\binom{\alpha}{\beta}$ tels que les deux équations sont compatibles.
On remarque que la deuxième ligne est le double de la première ($2L_1 = L_2$). Pour qu'une solution existe, on doit donc avoir :
\[ 2\alpha = \beta \iff 2\alpha - \beta = 0 \]
Ainsi, $\text{Im } A = \left\{ \begin{pmatrix} \alpha \\ \beta \end{pmatrix} \text{ tq } 2\alpha - \beta = 0 \right\}$.
On peut également exprimer cela en disant que l'image est l'orthogonal du vecteur $\binom{2}{-1}$ :
\[ \text{Im } A = \left\{ \begin{pmatrix} 2 \\ -1 \end{pmatrix} \right\}^\perp \]
\end{example}

\section{Sommes directes et Orthogonalité}

Pour une matrice rectangulaire $A \in \mathbb{C}^{N \times M}$, nous avons les propriétés de décomposition suivantes en somme directe :

\begin{proposition}
\begin{enumerate}
    \item $\mathbb{C}^N = \text{Im } A \oplus \text{Ker}(A^*)$
    \item $\mathbb{C}^M = \text{Im } A^* \oplus \text{Ker}(A)$
\end{enumerate}
Où $A^*$ désigne la matrice adjointe (transposée conjuguée) de $A$.
\end{proposition}

\begin{proof}
Démontrons que $\text{Ker } A = (\text{Im } A^*)^\perp$.

1. Montrons d'abord que $\text{Ker } A \subset (\text{Im } A^*)^\perp$ :
Soit $x \in \text{Ker } A$ (donc $Ax = 0$) et soit $y \in \text{Im } A^*$.
Par définition de l'image, il existe un vecteur $z$ tel que $y = A^*z$.
Calculons le produit scalaire $\langle x, y \rangle$ :
\begin{align*}
\langle x, y \rangle &= \langle x, A^*z \rangle \\
&= \langle Ax, z \rangle \\
&= \langle 0, z \rangle \\
&= 0
\end{align*}
Donc $x$ est orthogonal à tout élément de l'image de $A^*$.

2. Montrons ensuite que $(\text{Im } A^*)^\perp \subset \text{Ker } A$ :
Soit $x \in (\text{Im } A^*)^\perp$. Cela signifie que $\forall y \in \text{Im } A^*$, $\langle x, y \rangle = 0$.
En particulier, pour tout vecteur $z$, on a :
\begin{align*}
\langle x, A^*z \rangle &= 0 \\
\langle Ax, z \rangle &= 0, \quad \forall z
\end{align*}
Si le produit scalaire de $Ax$ avec tout vecteur $z$ est nul, alors nécessairement :
\[ Ax = 0 \]
Ce qui prouve que $x \in \text{Ker } A$.
\end{proof}

\section{Résolution Numérique : Pivot de Gauss}

Supposons $A$ inversible. La méthode du pivot de Gauss permet de transformer le système pour le rendre triangulaire.

\begin{example}
Considérons le système :
\[
\begin{cases}
x + 2y + 3z = 1 \\
4x + 5y + 6z = 2 \\
7x + 8y + 10z = 3
\end{cases}
\]
On effectue les opérations élémentaires sur les lignes :
\begin{align*}
L_2 &\leftarrow L_2 - 4L_1 \\
L_3 &\leftarrow L_3 - 7L_1
\end{align*}
Le système devient :
\[
\begin{cases}
x + 2y + 3z = 1 \\
0x + \dots = \dots \\
0x + \dots = \dots
\end{cases}
\rightarrow \dots \rightarrow
\begin{cases}
\dots \\
\dots \\
0x + 0y + z = \dots
\end{cases}
\]
\end{example}

La complexité algorithmique de cette méthode est en $O(N^3)$.

\section{Décompositions Matricielles}

\subsection{Décomposition LU}
La décomposition $A = LU$ consiste à écrire $A$ comme le produit d'une matrice triangulaire inférieure (Lower) $L$ et d'une matrice triangulaire supérieure (Upper) $U$.

Exemple de progression vers la forme triangulaire :
\[ A_1 = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 10 \end{pmatrix} \rightarrow A_2 = \begin{pmatrix} 1 & 2 & 3 \\ 0 & \times & \times \\ 0 & \times & \times \end{pmatrix} \rightarrow A_3 = \begin{pmatrix} 1 & 2 & 3 \\ 0 & \times & \times \\ 0 & 0 & \times \end{pmatrix} = U \]

Chaque étape correspond à une multiplication à gauche par une matrice de transfert $B_n$. Par exemple :
\[ A_2 = \begin{pmatrix} 1 & 0 & 0 \\ -4 & 1 & 0 \\ -7 & 0 & 1 \end{pmatrix} A_1 \]
On a alors $U = B_n B_{n-1} \dots B_1 A$, ce qui donne $A = B_1^{-1} B_2^{-1} \dots B_n^{-1} U = LU$.

\subsection{Aspects Pratiques}
En pratique, on utilise souvent une matrice de permutation $P$ pour assurer la stabilité numérique (choix du pivot), ce qui donne la décomposition :
\[ PA = LU \]
Le système $Ax = b$ devient $LUx = Pb$. On pose $y = Ux$, on résout $Ly = Pb$ par descente, puis $Ux = y$ par remontée.

\subsection{Décomposition QR (Gram-Schmidt)}
Pour une matrice $A \in \mathbb{C}^{N \times M}$ avec $M \le N$, on cherche $A = QR$ où $Q$ est une matrice dont les colonnes sont orthogonales ($Q^*Q = I_M$) et $R$ est triangulaire supérieure.
Les vecteurs $q_n$ sont obtenus par combinaison linéaire des colonnes $u_n$ de $A$ :
\begin{align*}
q_1 &= \frac{u_1}{\|u_1\|} \\
q_2 &= \frac{u_2 - \langle q_1, u_2 \rangle q_1}{\|u_2 - \langle q_1, u_2 \rangle q_1\|}
\end{align*}
On a la relation $Q = AR^{-1}$.

\subsection{Décomposition de Cholesky}
Si $A$ est une matrice symétrique définie positive, elle admet une décomposition de la forme :
\[ A = LL^T \]
où $L$ est une matrice triangulaire inférieure.

\section{Problème des Moindres Carrés}

\subsection{Formulation}
On cherche à ajuster un modèle linéaire $y = \alpha X + \beta$ sur un ensemble de points. On cherche $(\alpha, \beta) \in \mathbb{R}^2$ qui minimise la somme des carrés des erreurs :
\[ \min \sum_{i=1}^N |y_i - (\alpha X_i + \beta)|^2 \]
En posant l'erreur $e_i = y_i - (\alpha X_i + \beta)$, on cherche à minimiser $\|e\|^2 = \langle e, e \rangle$.

Sous forme matricielle, si on définit $e = y - A \binom{\alpha}{\beta}$ avec $A = \begin{pmatrix} X_1 & 1 \\ \vdots & \vdots \\ X_N & 1 \end{pmatrix}$, on veut minimiser :
\[ \left\| y - A \begin{pmatrix} \alpha \\ \beta \end{pmatrix} \right\|^2 \]

\subsection{Cas général et Formes Normales}
De façon générale, pour $A \in \mathbb{R}^{N \times M}$ et $b \in \mathbb{R}^N$ avec $N \gg M$, on cherche $x \in \mathbb{R}^M$ minimisant $\|Ax - b\|^2$.

Développement de la fonction à minimiser :
\begin{align*}
\|Ax - b\|^2 &= \langle Ax - b, Ax - b \rangle \\
&= (Ax)^T(Ax) - b^T Ax - (Ax)^T b + \|b\|^2 \\
&= x^T A^T A x - 2 (A^T b)^T x + \|b\|^2
\end{align*}

Pour trouver le minimum, on annule la dérivée par rapport à $x$ :
\begin{align*}
2 A^T A x - 2 (A^T b) &= 0 \\
2 A^T A x &= 2 A^T b
\end{align*}

On obtient ainsi les \textbf{formes normales} :
\[ A^T A x = A^T b \]

\end{document}